GENERAL:
  run_mode: inference

#EMBEDDER:
#  class: SentenceEmbedder
#  package: meteor.components.embeddings.embedding_models
#
#  TRAINER:
#    triplets_data_path:
#      - data/triples/triples_10.json
#      - data/triples/triples_20.json
#    batch_size: 32
#    pretrained_model: sentence-transformers/quora-distilbert-multilingual
#    model_save_path: models
#    n_samples: 5
#    batch_size: 128
#    epochs: 5
#    warmup_steps: 5000
#    evaluation_steps: 2000
#    weight_decay: 0.01
#    max_grad_norm: 1.0
#    use_amp: False
#    save_best_model: True
#    show_progress_bar: True

EMBEDDER:
  class: NaiveEmbedder
  package: meteor.components.embeddings.embedding_models
  TRAINER:
    # A data_config.json file
    data_config_path: data/data_config.json
    # The pretrained model
    #    pretrained_model: sentence-transformers/quora-distilbert-multilingual
    pretrained_model: vinai/phobert-base
    # Total of steps, it depends on the number of question answering pair that you have
    steps: 1000
    # When reach this step, we will be saved checkpoint
    save_steps: 10000
    # The max length of tokenizer
    max_length: 128
    # Use 20 for cossim, and 1 when you work with un-normalized embeddings with dot product
    scale: 20
    batch_size: 32
    model_save_path: models
    warmup_steps: 50
    datasets_per_batch: 2
    num_same_dataset: 1
    evaluation_steps: 200
    weight_decay: 0.01
    max_grad_norm: 1.0
    use_amp: False
    save_best_model: True
    show_progress_bar: True

EVALUATION:
  type: "evaluation_pipeline"
  corpus_name_or_path: "data/docs/corpus_docs.json"
  query_name_or_path: "data/docs/query_docs.json"
  model_name_or_path: [ "fschool-distilbert-multilingual-faq-v8.0.0" ]


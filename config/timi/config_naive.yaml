GENERAL:
  run_mode: inference

EMBEDDER:
  class: NaiveEmbedder
  package: saturn.components.embeddings.embedding_models
  TRAINER:
    # A data_config.json file
    data_config_path: data/train_data/timi/data_config.json
    # The pretrained model
    #    pretrained_model: sentence-transformers/quora-distilbert-multilingual
    pretrained_model: microsoft/MiniLM-L12-H384-uncased
    # Total of steps, it depends on the number of question answering pair that you have
    steps: 1250000
    # When reach this step, we will be saved checkpoint
    save_steps: 100000
    # The max length of tokenizer
    max_length: 64
    # Use 20 for cossim, and 1 when you work with un-normalized embeddings with dot product
    scale: 20
    batch_size: 128
    model_save_path: models
    warmup_steps: 1000
    datasets_per_batch: 2
    num_same_dataset: 1
    evaluation_steps: 200
    weight_decay: 0.01
    max_grad_norm: 1.0
    use_amp: False
    save_best_model: True
    show_progress_bar: True
    checkpoint_path: checkpoints
    checkpoint_save_step: 300000
    checkpoint_save_total_limit: 30
    resume_from_checkpoint: null

EVALUATION:
  corpus_name_or_path: "data/eval-data/timi/v2.0.0/corpus.json"
  query_name_or_path: "data/eval-data/timi/v2.0.0/query.json"
  model_name_or_path:
    - timi-idol-microsoft-MiniLM-L12-H384-uncased-faq-9M-v1.0.0
    - timi-idol-microsoft-MiniLM-L12-H384-uncased-faq-9M-v1.1.0

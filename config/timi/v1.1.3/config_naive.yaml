GENERAL:
  device: cuda
  project: timi
  version: v1.1.3
  output_data: assets
  output_model: models
  output_report: reports

  # Skipped signal
  skipped_gen_data: True
  skipped_training: True
  skipped_eval: False

DATA_GENERATION:
  skipped: True
  data_dir: data/timi/v1.1.3
  search_mode: bm25_embedder
  embedding_batch_size: 100
  neg_sim_batch_size: 200
  max_triple_per_file: 2000000
  EMBEDDER:
    cache_path: ./embedder_caches
    pretrained_name_or_abspath: timi-idol-keepitreal-vn-sbert-faq-9M-v1.0.0
#  COMBINER:


TRAINER:
  class: NaiveSemanticSimilarity
  package: saturn.components.embeddings.embedding_models
  data_config_path: data/train_data/timi/data_config.json
  # The pretrained model
  #    pretrained_model: sentence-transformers/quora-distilbert-multilingual
  pretrained_name_or_abspath: microsoft/MiniLM-L12-H384-uncased
  # Total of steps, it depends on the number of question answering pair that you have
  steps: 1250000
  # When reach this step, we will be saved checkpoint
  save_steps: 100000
  # The max length of tokenizer
  max_length: 64
  # Use 20 for cossim, and 1 when you work with un-normalized embeddings with dot product
  scale: 20
  batch_size: 128
  model_save_path: models
  warmup_steps: 1000
  datasets_per_batch: 2
  num_same_dataset: 1
  evaluation_steps: 200
  weight_decay: 0.01
  max_grad_norm: 1.0
  use_amp: False
  save_best_model: True
  show_progress_bar: True
  checkpoint_path: checkpoints
  checkpoint_save_step: 300000
  checkpoint_save_total_limit: 30
  resume_from_checkpoint: null

EVALUATION:
  type: "evaluation_pipeline"
  corpus_name_or_path: "data/eval-data/timi/v1.1.3/corpus.json"
  query_name_or_path: "data/eval-data/timi/v1.1.3/query.json"
  pretrained_name_or_abspath:
    - timi-idol-keepitreal-vn-sbert-faq-9M-v1.0.0

RELEASE:
  model_path: models
  pretrained_model: MiniLM-L12-H384-uncased
  data_size: 9M


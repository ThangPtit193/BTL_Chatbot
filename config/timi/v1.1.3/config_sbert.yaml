GENERAL:
  device: cuda:1
  project: timi
  version: v1.1.3_limit_10
  output_data: assets
  output_model: models
  output_report: reports

  # Skipped signal
  skipped_gen_data: False
  skipped_training: False
  skipped_eval: False

DATA_GENERATION:
  data_dir: data/timi/v1.1.3
  search_mode: bm25_embedder
  embedding_batch_size: 100
  neg_sim_batch_size: 200
  max_triple_per_file: 2000000
  max_sentence_repeated: 10
  EMBEDDER:
    cache_path: ./embedder_caches
    pretrained_name_or_abspath: timi-idol-keepitreal-vn-sbert-faq-9M-v1.0.0
#  COMBINER:


TRAINER:
  class: SBertSemanticSimilarity
  package: saturn.components.embeddings.embedding_models
  skipped: True
  pretrained_name_or_abspath: keepitreal/vietnamese-sbert
  model_save_path: models
  n_samples: 10
  batch_size: 256
  epochs: 20
  warmup_steps: 5000
  evaluation_steps: 2000
  weight_decay: 0.01
  max_grad_norm: 1.0
  use_amp: True
  save_best_model: True
  show_progress_bar: True
  checkpoint_path: checkpoints
  checkpoint_save_epoch: 3
  checkpoint_save_total_limit: 10
  save_by_epoch: 1
  # resume_from_checkpoint: checkpoints/epoch-14/checkpoint.pt

EVALUATION:
  type: "evaluation_pipeline"
  corpus_name_or_path: "data/eval-data/timi/v1.0.0/corpus.json"
  query_name_or_path: "data/eval-data/timi/v1.0.0/query.json"
  pretrained_name_or_abspath:
    #    - models_v1.1.3/epoch-1
    #    - models_v1.1.3/epoch-2
    #    - models_v1.1.3/epoch-3
    #    - models_v1.1.3/epoch-4
    #    - models_v1.1.3/epoch-5
    #    - models_v1.1.3/epoch-6
    #    - models_v1.1.3/epoch-7
    #    - models_v1.1.3/epoch-8
    #    - models_v1.1.3/epoch-9
    #    - models_v1.1.3/epoch-10
    #    - models_v1.1.3/epoch-11
    #    - models_v1.1.3/epoch-12
    #    - models_v1.1.3/epoch-13
    #    - models_v1.1.3/epoch-14
    #    - models_v1.1.3/epoch-15
    #    - models_v1.1.3/epoch-16
    #    - models_v1.1.3/epoch-17
    #    - models_v1.1.3/epoch-18
    #    - models_v1.1.3/epoch-19
    - timi-idol-keepitreal-vn-sbert-faq-9M-v1.0.0

RELEASE:
  model_path: models
  pretrained_model: MiniLM-L12-H384-uncased
  data_size: 9M
GENERAL:
  run_mode: inference

EMBEDDER:
  class: NaiveEmbedder
  package: saturn.components.embeddings.embedding_models
  TRAINER:
    # A data_config.json file
    data_config_path: data/train_data/dummy/data_config.json
    # The pretrained model
    #    pretrained_model: sentence-transformers/quora-distilbert-multilingual
    pretrained_name_or_abspath: vinai/phobert-base
    # Total of steps, it depends on the number of question answering pair that you have
    steps: 1000
    # When reach this step, we will be saved checkpoint
    save_steps: 1000
    # The max length of tokenizer
    max_length: 128
    # Use 20 for cossim, and 1 when you work with un-normalized embeddings with dot product
    scale: 20
    batch_size: 32
    model_save_path: models
    warmup_steps: 50
    datasets_per_batch: 2
    num_same_dataset: 1
    evaluation_steps: 200
    weight_decay: 0.01
    max_grad_norm: 1.0
    use_amp: False
    save_best_model: True
    show_progress_bar: True

EVALUATION:
  corpus_name_or_path: "data/eval-data/timi/corpus.json"
  query_name_or_path: "data/eval-data/timi/query.json"
  pretrained_name_or_abspath: ["distilbert-multilingual-faq-v3.2"]

